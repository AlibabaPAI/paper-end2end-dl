\section{Conclusions} \label{sec:conclusion}

Applying machine learning to compile-time and runtime optimizations requires
generating features first. This is a time consuming process, it needs
supervision by an expert, and even then we cannot be sure that the selected
features are optimal. In this paper we present a novel tool for building
optimization heuristics, DeepTune, which forgoes feature extraction entirely,
relying on powerful language modeling techniques to automatically build complex
and effective representations of programs directly from raw source code. The
result translates into a huge reduction in development effort, improved
heuristic performance, and more simple model designs.

Our approach is fully automated. Using DeepTune, compiler developers no longer
need to spend months using statistical methods and profile counters to select
program features via trial and error.  It is worth mentioning that we do not
tailor our model design or parameters for the optimization task at hand, yet we
achieve performance on par with and in most cases \emph{exceeding} state-of-the-
art predictive models.

We used DeepTune to automatically construct heuristics for two challenging
optimization problems: selecting the optimal execution device for OpenCL
kernels, and selecting OpenCL thread coarsening factors. In both cases, we
outperform state-of-the-art predictive models, achieving performance
improvements of 14\% and 12\%, respectively. We have also shown that the
DeepTune architecture allows us to exploit information learned from another
optimization problem to give the learning a boost. Doing so provides up to a
16\% performance improvement when training using a handful of training programs.
We suspect that this approach will be useful for other optimization tasks for
which training programs are a scarce resource.

In future work, we will extend our heuristic construction approach by
automatically learning dynamic features over raw data; apply unsupervised
learning techniques~\cite{Le2012} over unlabeled source code to further improve
learned representations of programs; and deploy trained DeepTune heuristic
models to low power embedded systems using optimization and compression of
neural networks~\cite{Han2015}.
